{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <61623A3D-DA3C-3AAD-B2F0-D363151DDB3F> /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <EACD001F-FCB9-380E-AD73-D522177FC040> /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af632f3a55974a2db3ee866cc961e644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359c634006cc4eeab0b1e1bb4c0bd3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"tiiuae/falcon-7b-instruct\"  # Corrected model name\n",
    "\n",
    "# Set up offload folder to store parts of the model that won't fit in GPU memory\n",
    "offload_folder = \"./offload\"  \n",
    "os.makedirs(offload_folder, exist_ok=True)\n",
    "\n",
    "# Load the Falcon model with offloading\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cpu\",\n",
    "    offload_folder=offload_folder,  # Specify the offload folder for offloaded weights\n",
    "    torch_dtype=torch.float16  # Using mixed precision to reduce memory load\n",
    ")\n",
    "\n",
    "# The rest of your model code follows...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_llm_model():\n",
    "    prompt = \"What is artificial intelligence?\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")  # Tokenize the input prompt\n",
    "\n",
    "    # Create an attention mask\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)  # Create a mask of ones since there's no padding in the input\n",
    "\n",
    "    # Generate text using the model\n",
    "    with torch.no_grad():  # No gradients needed for inference\n",
    "        generated_output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,  # Provide the attention mask to avoid warnings\n",
    "            max_length=100,  # Limit response length for test purposes\n",
    "            do_sample=True,  # Enable sampling for variety\n",
    "            temperature=0.7,  # Control creativity\n",
    "            pad_token_id=tokenizer.eos_token_id  # Set pad token ID explicitly\n",
    "        )\n",
    "    \n",
    "    # Decode and print the output\n",
    "    output_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    print(\"Generated Response:\\n\", output_text)\n",
    "\n",
    "# Run the test function to see if the model is working\n",
    "test_llm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Function to polish the resume\n",
    "def polish_resume(position_name, resume_content, polish_prompt=\"\"):\n",
    "    if polish_prompt and polish_prompt.strip():\n",
    "        prompt_use = f\"Given the resume content: '{resume_content}', polish it based on the following instructions: {polish_prompt} for the {position_name} position.\"\n",
    "    else:\n",
    "        prompt_use = f\"Suggest improvements for the following resume content: '{resume_content}' to better align with the requirements and expectations of a {position_name} position. Return the polished version, highlighting necessary adjustments for clarity, relevance, and impact in relation to the targeted role.\"\n",
    "\n",
    "    generated_response = model.generate(\n",
    "        tokenizer.encode(prompt_use, return_tensors=\"pt\"),\n",
    "        max_length=512,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return tokenizer.decode(generated_response[0], skip_special_tokens=True)\n",
    "\n",
    "resume_polish_application = gr.Interface(\n",
    "    fn=polish_resume,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Position Name\", placeholder=\"Enter the name of the position...\"),\n",
    "        gr.Textbox(label=\"Resume Content\", placeholder=\"Paste your resume content here...\", lines=20),\n",
    "        gr.Textbox(label=\"Polish Instruction (Optional)\", placeholder=\"Enter specific instructions or areas for improvement (optional)...\", lines=2),\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Polished Content\"),\n",
    "    title=\"Resume Polish Application\",\n",
    "    description=\"This application helps you polish your resume. Enter the position you want to apply, your resume content, and specific instructions or areas for improvement (optional), then get a polished version of your content.\"\n",
    ")\n",
    "\n",
    "resume_polish_application.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cover_letter(position_name, company_name, personal_experience, key_skills=\"\"):\n",
    "    prompt = (\n",
    "        f\"I am applying for the position of {position_name} at {company_name}. \"\n",
    "        f\"Below are my relevant experiences: {personal_experience}. \"\n",
    "    )\n",
    "    if key_skills and key_skills.strip():\n",
    "        prompt += f\"Additionally, I possess the following key skills that are valuable for this role: {key_skills}. \"\n",
    "    prompt += \"Please generate a compelling cover letter based on this information that highlights my skills, experiences, and interest in the company.\"\n",
    "\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        generated_output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask, \n",
    "            max_length=512,  \n",
    "            do_sample=True, \n",
    "            temperature=0.7,  \n",
    "            pad_token_id=tokenizer.eos_token_id  \n",
    "        )\n",
    "    \n",
    "   \n",
    "    output_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "    return output_text\n",
    "\n",
    "\n",
    "cover_letter_app = gr.Interface(\n",
    "    fn=generate_cover_letter,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Position Name\", placeholder=\"Enter the name of the position...\"),\n",
    "        gr.Textbox(label=\"Company Name\", placeholder=\"Enter the name of the company...\"),\n",
    "        gr.Textbox(label=\"Personal Experience\", placeholder=\"Describe your relevant experiences...\", lines=5),\n",
    "        gr.Textbox(label=\"Key Skills (Optional)\", placeholder=\"Enter any specific skills you want to highlight (optional)...\", lines=2),\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Generated Cover Letter\"),\n",
    "    title=\"Cover Letter Generator\",\n",
    "    description=\"This application helps you generate a cover letter based on the position, company, your relevant experiences, and any key skills you want to highlight. Enter the required information and get a tailored cover letter.\"\n",
    ")\n",
    "\n",
    "\n",
    "cover_letter_app.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
